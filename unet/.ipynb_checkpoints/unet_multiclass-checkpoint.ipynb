{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Set some parameters\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')\n",
    "seed = 42\n",
    "random.seed = seed\n",
    "np.random.seed = seed\n",
    "TEST_PATH = '../../PennFudanPed/TEST/'\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_ids = next(os.walk(TEST_PATH + 'PNGImages'))[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ncoco_mask_path = '../../coco_persons/' + 'masks'\\ntrain_coco_mask_ids = next(os.walk(coco_mask_path))[2]\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco_persons_img_path = '../../multimedia_datasets/person/' + 'images'\n",
    "coco_persons_mask_path = '../../multimedia_datasets/person/' + 'masks'\n",
    "\n",
    "coco_knifes_img_path = '../../multimedia_datasets/knife/' + 'images'\n",
    "coco_knifes_mask_path = '../../multimedia_datasets/knife/' + 'masks'\n",
    "\n",
    "coco_cars_img_path = '../../multimedia_datasets/car/' + 'images'\n",
    "coco_cars_mask_path = '../../multimedia_datasets/car/' + 'masks'\n",
    "'''\n",
    "coco_mask_path = '../../coco_persons/' + 'masks'\n",
    "train_coco_mask_ids = next(os.walk(coco_mask_path))[2]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IoU metric\n",
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)\n",
    "# Define IoU metric\n",
    "def iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in [0.5]:\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coco_images(X_train, Y_train, image_path, mask_path, pos):\n",
    "    print(image_path)\n",
    "    idsy = next(os.walk(image_path))[2]\n",
    "    for n, id_ in tqdm_notebook(enumerate(idsy), total=len(idsy)):\n",
    "        if 1:#n%200==0:\n",
    "            path = image_path + \"/\"\n",
    "            path_masks = mask_path + \"/\"\n",
    "            if Path(path+id_).exists and Path(path_masks + id_).exists:\n",
    "                try:\n",
    "                    img = imread(path+id_)[:,::IMG_CHANNELS]\n",
    "                except IndexError as e:\n",
    "                    print(e)\n",
    "                    if Path(path+id_).exists:\n",
    "                        os.remove(path+id_)\n",
    "\n",
    "                    if Path(path_masks + id_).exists:    \n",
    "                        os.remove(path_masks+id_)\n",
    "                    continue\n",
    "\n",
    "                img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "                try:\n",
    "                    X_train.append(img)\n",
    "                except ValueError as v:\n",
    "                    print(v)\n",
    "                    if Path(path+id_).exists:\n",
    "                        os.remove(path+id_)\n",
    "                    if Path(path_masks + id_).exists: \n",
    "                        os.remove(path_masks+id_)\n",
    "                    continue\n",
    "                mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                mask = imread(path_masks+id_[:-4]+'.png')\n",
    "\n",
    "                mask = resize(mask , (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
    "                                                  preserve_range=True)\n",
    "                mask = np.expand_dims(mask, axis=2)\n",
    "                mask = np.where(mask<0.0001, 0, 1)\n",
    "                zero = np.zeros(mask.shape)\n",
    "                #mask = np.ones(mask.shape)\n",
    "                if pos == 0:\n",
    "                    ext_mask = np.concatenate((mask, zero, zero), axis=2)\n",
    "                elif pos == 1:\n",
    "                    ext_mask = np.concatenate((zero, mask, zero), axis=2)\n",
    "                else:\n",
    "                    ext_mask = np.concatenate((zero, zero, mask), axis=2)\n",
    "                Y_train.append(ext_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../multimedia_datasets/person/images/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359e28f5b7a343ffae9bd371d52a7750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=53137), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get and resize train images and masks\n",
    "#X_train_coco = np.zeros((len(train_coco_img_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "#Y_train_coco = np.zeros((len(train_coco_img_ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
    "X_train_persons, Y_train_persons = [], []\n",
    "load_coco_images(X_train_persons, Y_train_persons, coco_persons_img_path+'/', coco_persons_mask_path, 0)\n",
    "X_train_persons = np.asarray(X_train_persons, dtype=np.int32)\n",
    "print(X_train_persons.shape)\n",
    "Y_train_persons = np.asarray(Y_train_persons)\n",
    "print(Y_train_persons.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_knifes, Y_train_knifes = [], []\n",
    "load_coco_images(X_train_knifes, Y_train_knifes, coco_knifes_img_path, coco_knifes_mask_path, 1)\n",
    "X_train_knifes = np.asarray(X_train_knifes, dtype=np.int32)\n",
    "print(X_train_knifes.shape)\n",
    "Y_train_knifes = np.asarray(Y_train_knifes)\n",
    "print(Y_train_knifes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cars, Y_train_cars = [], []\n",
    "load_coco_images(X_train_cars, Y_train_cars, coco_cars_img_path, coco_cars_mask_path, 2)\n",
    "X_train_cars = np.asarray(X_train_cars, dtype=np.int32)\n",
    "print(X_train_cars.shape)\n",
    "Y_train_cars = np.asarray(Y_train_cars)\n",
    "print(Y_train_cars.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train_persons, X_train_knifes, X_train_cars), axis=0)\n",
    "Y_train = np.concatenate((Y_train_persons, Y_train_knifes, Y_train_cars), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and resize test images\n",
    "X_test = np.zeros((len(test_img_ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
    "sizes_test = []\n",
    "print(len(test_img_ids))\n",
    "print('Getting and resizing test images ... ')\n",
    "sys.stdout.flush()\n",
    "for n, id_ in tqdm(enumerate(test_img_ids), total=len(test_img_ids)):\n",
    "    path = TEST_PATH + 'PNGImages' + '/' + id_\n",
    "    img = imread(path)[:,:,:IMG_CHANNELS]\n",
    "    sizes_test.append([img.shape[0], img.shape[1]])\n",
    "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
    "    X_test[n] = img\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if training data looks all right\n",
    "ix = random.randint(0, len(X_train))\n",
    "imshow(X_train[ix])\n",
    "plt.show()\n",
    "imshow(Y_train[ix]*255.0)\n",
    "plt.show()\n",
    "print(Y_train[ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build U-Net model\n",
    "inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "s = Lambda(lambda x: x / 255) (inputs)\n",
    "\n",
    "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
    "c1 = Dropout(0.1) (c1)\n",
    "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name=\"c1\") (c1)\n",
    "p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name=\"c2\") (p1)\n",
    "c2 = Dropout(0.1) (c2)\n",
    "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
    "p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
    "c3 = Dropout(0.2) (c3)\n",
    "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name=\"c3\") (c3)\n",
    "p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
    "c4 = Dropout(0.2) (c4)\n",
    "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name=\"c4\") (c4)\n",
    "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
    "c5 = Dropout(0.3) (c5)\n",
    "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name=\"c5\") (c5)\n",
    "\n",
    "u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same', name=\"u6\") (c5)\n",
    "u6 = concatenate([u6, c4])\n",
    "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
    "c6 = Dropout(0.2) (c6)\n",
    "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name=\"c6\") (c6)\n",
    "\n",
    "u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = concatenate([u7, c3])\n",
    "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name=\"c7\") (u7)\n",
    "c7 = Dropout(0.2) (c7)\n",
    "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
    "\n",
    "u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same', name=\"u8\") (c7)\n",
    "u8 = concatenate([u8, c2])\n",
    "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name=\"c8\") (u8)\n",
    "c8 = Dropout(0.1) (c8)\n",
    "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
    "\n",
    "u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same', name=\"u9\") (c8)\n",
    "u9 = concatenate([u9, c1], axis=3)\n",
    "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same', name=\"c9\") (u9)\n",
    "c9 = Dropout(0.1) (c9)\n",
    "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
    "\n",
    "outputs = Conv2D(num_classes, (1, 1), activation='sigmoid') (c9)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[outputs])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[mean_iou,iou])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from keras.callbacks import TensorBoard\n",
    "#tensorboard = TensorBoard(log_dir=\"logs/\",histogram_freq=1,\n",
    "                            write_graph=False, write_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "earlystopper = EarlyStopping(patience=3, verbose=1)\n",
    "#checkpointer = ModelCheckpoint('../../modele/test.h5', verbose=1, save_best_only=True)\n",
    "results = model.fit(X_train, Y_train, validation_split=0.1, batch_size=8, epochs=50, \n",
    "                    callbacks=[earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../modele/test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Predict on train, val and test\n",
    "model = load_model('../../modele/test.h5', custom_objects={'mean_iou': mean_iou,'iou': iou})\n",
    "preds_train = model.predict(X_train[:int(X_train.shape[0]*0.9)], verbose=1)\n",
    "preds_val = model.predict(X_train[int(X_train.shape[0]*0.9):], verbose=1)\n",
    "preds_test = model.predict(X_test, verbose=1)\n",
    "\n",
    "# Threshold predictions\n",
    "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
    "preds_val_t = (preds_val > 0.5).astype(np.uint8)\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
    "\n",
    "# Create list of upsampled test masks\n",
    "preds_test_upsampled = []\n",
    "for i in range(len(preds_test)):\n",
    "    preds_test_upsampled.append(resize(np.squeeze(preds_test[i]), \n",
    "                                       (sizes_test[i][0], sizes_test[i][1]), \n",
    "                                       mode='constant', preserve_range=True))\n",
    "    \n",
    "# Perform a sanity check on some random training samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ON TRAINING DATA\")\n",
    "ix = random.randint(0, len(preds_train_t))\n",
    "print(X_train[ix].dtype )\n",
    "print(X_train[ix].shape)\n",
    "plt.imshow(X_train[ix]);  plt.show(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Y_train[ix]*255) ;  plt.show(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = preds_train_t[ix]*255.0\n",
    "a = a.astype(np.int32)\n",
    "print(np.where(preds_train_t[ix]>0.0))\n",
    "plt.imshow(a) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ON VALIDATION\")\n",
    "ix = random.randint(0, len(preds_val_t))\n",
    "imshow(X_train[int(X_train.shape[0]*0.9):][ix]) ; plt.show() ;\n",
    "imshow(Y_train[int(Y_train.shape[0]*0.9):][ix]*255) ; plt.show() ; \n",
    "imshow(preds_val_t[ix]*255) ; plt.show() ; \n",
    "# Perform a sanity check on some random test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ON RANDOM TEST\")\n",
    "ix = random.randint(0, len(preds_test_t)-1)\n",
    "\n",
    "imshow(X_test[ix]) ; plt.show(); \n",
    "imshow(preds_test_t[ix]*255) ; plt.show() ; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
